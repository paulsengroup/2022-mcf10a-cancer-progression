#!/usr/bin/env python3

# Copyright (C) 2023 Roberto Rossini <roberros@uio.no>
#
# SPDX-License-Identifier: MIT

import argparse
import itertools
import logging
import multiprocessing as mp
import os
import pathlib
import warnings
from typing import Dict, List, Tuple

import bioframe as bf
import numpy as np
import pandas as pd
import pyBigWig


def make_cli():
    cli = argparse.ArgumentParser()

    def existing_file(arg):
        if (path := pathlib.Path(arg)).exists():
            return path

        raise FileNotFoundError(f'File "{arg}" does not exists')

    cli.add_argument(
        "bedgraph",
        type=existing_file,
        help="Path to the (sub)compartment bedGraph generated by dcHiC.",
    )

    cli.add_argument(
        "chip-table",
        type=existing_file,
        help="Path to a TSV with the file and metadata for plotting.",
    )

    cli.add_argument(
        "output-prefix",
        type=pathlib.Path,
        help="Path to output prefix (including parent folder(s) but without extension).",
    )

    cli.add_argument(
        "--labels",
        type=str,
        required=True,
        help="Comma-separated list of condition labels to use when plotting.\n"
        "Should be in the same order as conditions (i.e. cell types) found in the bedGraph generated by dcHiC.",
    )

    cli.add_argument(
        "--nproc",
        type=int,
        choices=range(1, mp.cpu_count() + 1),
        default=mp.cpu_count(),
        help="Maximum number of parallel processes.",
    )

    cli.add_argument(
        "--coverage-only",
        action="store_true",
        default=False,
        help="Use coverage information only (i.e. do not read signal from bigwig file(s)).",
    )

    cli.add_argument(
        "--path-prefix",
        type=pathlib.Path,
        default=pathlib.Path(os.getcwd()),
        help="Path prefix for file(s) listed in the ChIP table.",
    )

    cli.add_argument(
        "--force",
        action="store_true",
        default=False,
        help="Force overwrite existing files.",
    )

    return cli


def handle_path_collisions(*paths: pathlib.Path) -> None:
    collisions = [p for p in paths if p.exists()]

    if len(collisions) != 0:
        collisions = "\n - ".join((str(p) for p in collisions))
        raise RuntimeError(
            "Refusing to overwrite file(s):\n" f" - {collisions}\n" "Pass --force to overwrite existing file(s)."
        )


def import_subcomps(path_to_bedgraph: pathlib.Path) -> pd.DataFrame:
    """
    Read a bedgraph into a df with the following columns:
    chrom, start, end, padj, *.state
    where *.state stores the subcompartment label for the given interval.
    """
    logging.info("Reading subcompartments from %s...", path_to_bedgraph)
    df = pd.read_table(path_to_bedgraph).rename(columns={"chr": "chrom"})
    df1 = df[["chrom", "start", "end", "padj"]].copy()
    df2 = df.filter(regex=".state$")

    df1[df2.columns] = df2
    logging.info("Imported %d subcompartments", len(df1))
    return df1


def read_markers_table(path_to_table: pathlib.Path, prefix: pathlib.Path) -> pd.DataFrame:
    """
    Read the markers table into a dataframe like the following
                        peaks                                      bigwig
    marker   condition
    CTCF     MCF10CA1a  [/prefix/peak1.bed, /prefix/peak2.bed...]  [/prefix/bwig1.bw, /prefix/bwig2.bw...]
             MCF10AT1   [...]                                      [...]
             MCF10A     [...]                                      [...]
    H2A.ZH   MCF10CA1a  [/prefix/peak1.bed, /prefix/peak2.bed...]  [/prefix/[bwig1.bw, /prefix/bwig2.bw...]
    ...
    """

    logging.info("Reading marker file table from %s...", path_to_table)
    df = pd.read_table(path_to_table).fillna("")

    logging.info("Read %d markers across %d conditions...", len(df["marker"].unique()), len(df["condition"].unique()))

    # Group replicates (i.e. rows with the same marker)
    # Also group by condition (i.e. cell type)
    df = df.groupby(["marker", "condition"]).aggregate(list)

    # Add prefix to file paths
    def fx(paths_in: List[str]) -> List[str]:
        paths_out = []
        for p in paths_in:
            if p == "":
                paths_out.append("")
            else:
                paths_out.append(str(prefix / p))

        return paths_out

    # Add path prefix. Does nothing if paths are already absolute
    return df.applymap(fx)


def generate_label_mappings(subcomps: pd.DataFrame, labels: List[str]) -> Dict[str, str]:
    """
    Map pretty labels specified through the CLI to labels from the subcompartment bedGraph.
    Mapping is based on label order.
    """
    original_labels = subcomps.filter(regex=r".state$").columns.tolist()
    if len(original_labels) != len(labels):
        raise RuntimeError(f"Expected {len(original_labels)} labels, found {len(labels)}")

    mappings = {k.removesuffix(".state"): v for k, v in zip(original_labels, labels)}
    logging.info("Label mappings: %s", mappings)
    return mappings


def rename_subcompartment_conditions(subcomps: pd.DataFrame, label_mappings: Dict[str, str]) -> pd.DataFrame:
    logging.info("Renaming conditions in subcompartment df...")
    df = subcomps.rename(columns={f"{k}.state": v for k, v in label_mappings.items()})
    df = df.rename(columns={f"{k}.freq": v for k, v in label_mappings.items()})
    return df


def rename_marker_conditions(marker_table: pd.DataFrame, label_mappings: Dict[str, str]) -> pd.DataFrame:
    logging.info("Renaming conditions in marker df...")
    df = marker_table.copy().reset_index()

    df["condition"] = df["condition"].map(label_mappings)
    assert not df["condition"].hasnans

    return df.set_index(marker_table.index.names)


def init_table(subcomps: pd.DataFrame, markers: Tuple[str], condition_prefix: str = "MCF10") -> pd.DataFrame:
    """
    Init a dataframe with chrom, start, end and as row index.
    Columns are indexed using two levels, one for cell type and another for markers and subcompartment labels
    Example:

                            MCF10A                     ... MCF10CA1a                  ...
                            subcompartment CTCF H2A.ZH ... subcompartment CTCF H2A.ZH ...
    chrom start end                                    ...                            ...
    chr1  40000     50000   A0             NaN  NaN    ... A0             NaN  NaN    ...
          50000     60000   A0             NaN  NaN    ... A0             NaN  NaN    ...
    """

    logging.info("Initializing df...")

    conditions = [col for col in subcomps.columns if col.startswith(condition_prefix)]
    labels = ["subcompartment"] + list(markers)
    cols = pd.MultiIndex.from_product([conditions, labels])

    subcomps = subcomps.copy().set_index(["chrom", "start", "end"])

    df = pd.DataFrame(index=subcomps.index, columns=cols, dtype=float)

    for cond in conditions:
        df.loc[:, (cond, "subcompartment")] = subcomps[cond]

    return df


def overlap_bigwig_with_peaks_helper(
    marker: str,
    condition: str,
    peak_files: List[pathlib.Path],
    bigwig_files: List[pathlib.Path],
    method: str,
    filler_value=1.0,
) -> Tuple[str, str, List[pd.DataFrame]]:
    """
    Loop over pairs of peak/bigwig files and assign values read from bigwig files to peaks.
    For each pair of files, return a DataFrame with the intervals with their score.
    """
    dfs = []

    logging.info("[%s] [%s] Begin reading values...", condition, marker)

    for peak_file, bigwig_file in itertools.zip_longest(peak_files, bigwig_files, fillvalue=None):
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            # Suppress warnings about potential data loss
            df = bf.read_table(peak_file, schema="bed3")

        # Bigwig file is missing
        if bigwig_file == "" or bigwig_file is None:
            df["score"] = filler_value
            dfs.append(df)
            continue

        with pyBigWig.open(bigwig_file) as bwf:
            df["score"] = df.apply(
                lambda row: bwf.stats(row["chrom"], row["start"], row["end"], method)[0], axis="columns"
            )

        dfs.append(df)

    logging.info("[%s] [%s] DONE reading values!", condition, marker)
    return marker, condition, dfs


def overlap_bigwigs_with_peaks(
    marker_file_table: pd.DataFrame, nproc: int, method: str = "sum"
) -> List[Tuple[str, str, List[pd.DataFrame]]]:
    """
    Return a list of tuples of size 3, with the following fields:
        - marker: str
        - condition: str
        - dfs: List[pd.DataFrame]

    Each df contains the list of peaks with their score, where score was computed
    using the method specified as param (i.e. by default, summing bigwig values overlapping a peak)
    """
    with mp.Pool(nproc) as pool:
        return pool.starmap(
            overlap_bigwig_with_peaks_helper,
            zip(
                marker_file_table.index.get_level_values("marker"),
                marker_file_table.index.get_level_values("condition"),
                marker_file_table["peaks"],
                marker_file_table["bigwig"],
                itertools.repeat(method),
            ),
            chunksize=1,
        )


def overlap_peaks_with_subcompartments(
    subcompartent_coords: pd.DataFrame, peak_dfs: List[pd.DataFrame], aggregation_method: str = "sum"
) -> np.ndarray:
    """
    Overlap peaks with subcompartments aggregating values with the method specified through aggregation_method.
    Return a numpy array with one value for each interval
    """
    assert aggregation_method in {"sum", "mean", "median"}

    # Overlap subcompartments with peak_dfs
    overlaps = pd.concat(
        [bf.overlap(subcompartent_coords, df)[["chrom", "start", "end", "score_"]] for df in peak_dfs]
    ).rename(columns=lambda c: c.rstrip("_"))
    overlaps["score"] = np.nan_to_num(overlaps["score"])

    # Aggregate scores to deal with subcompartment intervals overlapping multiple peaks
    if aggregation_method == "sum":
        overlaps = overlaps.groupby(by=["chrom", "start", "end"]).sum(numeric_only=True).reset_index()
    elif aggregation_method == "mean":
        overlaps = overlaps.groupby(by=["chrom", "start", "end"]).mean(numeric_only=True).reset_index()
    else:
        assert aggregation_method == "median"
        overlaps = overlaps.groupby(by=["chrom", "start", "end"]).median(numeric_only=True).reset_index()

    # Vector is empty: assume data is missing
    if (overlaps["score"] == 0).all():
        return np.full_like(overlaps["score"], np.nan)

    return overlaps["score"].to_numpy()


def fill_table(df: pd.DataFrame, file_table: pd.DataFrame, nproc: int) -> pd.DataFrame:
    """
    Fill df with data read from peak/bigwig files listed in file_table
    """
    logging.info("Filling df...")
    coords = df.index.to_frame().reset_index(drop=True)
    for marker, condition, dfs in overlap_bigwigs_with_peaks(file_table, nproc):
        logging.info("[%s] [%s] Overlapping peaks with subcompartments...", condition, marker)
        df.loc[:, (condition, marker)] = overlap_peaks_with_subcompartments(coords, dfs)

    return df


def setup_logger(level=logging.INFO):
    fmt = "[%(asctime)s] %(levelname)s: %(message)s"
    logging.basicConfig(format=fmt)
    logging.getLogger().setLevel(level)


def main():
    args = vars(make_cli().parse_args())

    output_prefix = args["output-prefix"]

    if not args["force"]:
        handle_path_collisions(*(output_prefix.with_suffix(ext) for ext in (".pickle", ".tsv")))

    subcomps = import_subcomps(args["bedgraph"])
    marker_file_table = read_markers_table(args["chip-table"], args["path_prefix"])

    if args["coverage_only"]:
        # Drop bigwig files from file table
        marker_file_table["bigwig"] = [[]] * len(marker_file_table)

    label_mappings = generate_label_mappings(subcomps, args["labels"].split(","))
    subcomps = rename_subcompartment_conditions(subcomps, label_mappings)
    marker_file_table = rename_marker_conditions(marker_file_table, label_mappings)

    markers = tuple(marker_file_table.index.get_level_values("marker").unique().tolist())

    df = init_table(subcomps, markers)
    df = fill_table(df, marker_file_table, args["nproc"])

    logging.info("Writing table to %s.{pickle,tsv}...", output_prefix)
    output_prefix.parent.mkdir(exist_ok=True, parents=True)
    df.to_pickle(output_prefix.with_suffix(".pickle"))
    df.to_csv(output_prefix.with_suffix(".tsv"), sep="\t")


if __name__ == "__main__":
    setup_logger()
    main()
